"""
çŸ¥è¯†é—®ç­”æœåŠ¡ - ã€æ ¸å¿ƒæ–‡ä»¶ã€‘
~~~~~~~~~~~

ç‹¬ç«‹çš„çŸ¥è¯†é—®ç­”æœåŠ¡ï¼Œæ”¯æŒï¼š
- ä»çŸ¥è¯†åº“æ–‡æ¡£å’Œæ•°æ®åº“FAQä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
- åŸºäºæ£€ç´¢ç»“æœè¿›è¡Œé—®ç­”
- å…³é”®è¯åŒ¹é…å’Œç›¸ä¼¼åº¦è®¡ç®—
- æ”¯æŒæ‰‹åŠ¨å’Œè‡ªåŠ¨å…³é”®è¯æå–
- å¯ç‹¬ç«‹è¿è¡Œæˆ–é›†æˆåˆ°å¯¹è¯æœåŠ¡

æ ¸å¿ƒåŠŸèƒ½ï¼š
- search_knowledge(): çŸ¥è¯†æ£€ç´¢ï¼ˆæ”¯æŒåŒæ•°æ®æºï¼‰
- answer_question(): åŸºäºæ£€ç´¢ç»“æœçš„é—®ç­”
- _load_from_files(): ä»Markdownæ–‡ä»¶åŠ è½½çŸ¥è¯†
- _load_from_database(): ä»æ•°æ®åº“FAQè¡¨åŠ è½½çŸ¥è¯†
- _calculate_similarity(): ç›¸ä¼¼åº¦è®¡ç®—ç®—æ³•

æ•°æ®æºï¼š
- Markdownæ–‡æ¡£ï¼šdoc/knowledge_slices/
- æ•°æ®åº“FAQï¼šfaq_list å’Œ faq_list_keys è¡¨

ä½œè€…: æ™ºç³–å›¢é˜Ÿ
æ—¥æœŸ: 2025-01-21
"""

import os
import re
import json
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
from utils.logger import get_logger
from utils.config_loader import get_config
from utils.database import get_db_connection, execute_query

logger = get_logger(__name__)


class KnowledgeQAService:
    """çŸ¥è¯†é—®ç­”æœåŠ¡ç±»"""

    def __init__(self, knowledge_base_path: Optional[str] = None, load_from_db: bool = True):
        """
        åˆå§‹åŒ–çŸ¥è¯†é—®ç­”æœåŠ¡

        Args:
            knowledge_base_path: çŸ¥è¯†åº“æ–‡æ¡£è·¯å¾„ï¼Œé»˜è®¤ä¸º doc/knowledge_slices/
            load_from_db: æ˜¯å¦ä»æ•°æ®åº“åŠ è½½FAQï¼Œé»˜è®¤True
        """
        # è·å–çŸ¥è¯†åº“è·¯å¾„
        if knowledge_base_path:
            self.knowledge_base_path = Path(knowledge_base_path)
        else:
            # é»˜è®¤è·¯å¾„ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ doc/knowledge_slices/
            current_dir = Path(__file__).parent.parent.parent
            self.knowledge_base_path = current_dir / "doc" / "knowledge_slices"

        # çŸ¥è¯†åº“ç¼“å­˜
        self.knowledge_base: List[Dict[str, Any]] = []
        self.is_loaded = False
        self.load_from_db = load_from_db

        # åŠ è½½çŸ¥è¯†åº“
        self._load_knowledge_base()

        logger.info(f"âœ… çŸ¥è¯†é—®ç­”æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(self.knowledge_base)} æ¡çŸ¥è¯†")
    
    def _load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“ï¼ˆæ–‡æ¡£ + æ•°æ®åº“ï¼‰"""
        total_qa = 0

        # ä»æ•°æ®åº“åŠ è½½ï¼ˆä¼˜å…ˆï¼‰
        if self.load_from_db:
            db_count = self._load_from_database()
            total_qa += db_count
            logger.info(f"ğŸ“Š ä»æ•°æ®åº“åŠ è½½äº† {db_count} æ¡FAQ")

        # ä»æ–‡æ¡£åŠ è½½
        file_count = self._load_from_files()
        total_qa += file_count
        logger.info(f"ğŸ“ ä»æ–‡æ¡£åŠ è½½äº† {file_count} æ¡é—®ç­”")

        self.is_loaded = True
        logger.info(f"ğŸ“š çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œæ€»é—®ç­”æ•°: {total_qa}")

    def _load_from_files(self) -> int:
        """ä»æ–‡æ¡£åŠ è½½çŸ¥è¯†åº“"""
        try:
            if not self.knowledge_base_path.exists():
                logger.warning(f"âš ï¸ çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {self.knowledge_base_path}")
                return 0

            # è·å–æ‰€æœ‰markdownæ–‡ä»¶
            md_files = list(self.knowledge_base_path.glob("*.md"))
            logger.info(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ä¸ªçŸ¥è¯†åº“æ–‡æ¡£")

            # è§£ææ¯ä¸ªæ–‡æ¡£
            file_count = 0
            for md_file in sorted(md_files):
                try:
                    qa_pairs = self._parse_knowledge_file(md_file)
                    self.knowledge_base.extend(qa_pairs)
                    file_count += len(qa_pairs)
                    logger.info(f"âœ… åŠ è½½æ–‡æ¡£ {md_file.name}: {len(qa_pairs)} æ¡é—®ç­”")
                except Exception as e:
                    logger.error(f"âŒ è§£ææ–‡æ¡£ {md_file.name} å¤±è´¥: {str(e)}")

            return file_count

        except Exception as e:
            logger.error(f"âŒ ä»æ–‡æ¡£åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return 0

    def _load_from_database(self) -> int:
        """ä»æ•°æ®åº“åŠ è½½FAQ"""
        try:
            import pymysql

            # ç›´æ¥è¿æ¥æ•°æ®åº“ï¼ˆé¿å…è¿æ¥æ± é—®é¢˜ï¼‰
            conn = pymysql.connect(
                host='115.120.251.86',
                port=3306,
                user='root',
                password='MyNewPass!2024',
                database='ai',
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )

            with conn.cursor() as cursor:
                # æŸ¥è¯¢å¯ç”¨çš„FAQ
                sql = """
                    SELECT
                        f.id,
                        f.question,
                        f.answer,
                        f.category,
                        f.source,
                        f.status,
                        f.sort_order,
                        f.view_count,
                        f.like_count,
                        f.is_manual,
                        f.description,
                        GROUP_CONCAT(
                            CONCAT(k.keyword, ':', k.keyword_type, ':', k.weight)
                            ORDER BY k.weight DESC, k.keyword
                        ) as keywords_str
                    FROM faq_list f
                    LEFT JOIN faq_list_keys k ON f.id = k.faq_id
                    WHERE f.status = 1
                    GROUP BY f.id
                    ORDER BY f.sort_order, f.id
                """

                cursor.execute(sql)
                faq_records = cursor.fetchall()

                for record in faq_records:
                    # è§£æå…³é”®è¯
                    keywords = []
                    keywords_str = record.get('keywords_str')
                    if keywords_str:
                        for kw_str in keywords_str.split(','):
                            if ':' in kw_str:
                                keyword, kw_type, weight = kw_str.split(':', 2)
                                keywords.append({
                                    'keyword': keyword,
                                    'type': kw_type,
                                    'weight': float(weight)
                                })

                    # æ„å»ºé—®ç­”å¯¹
                    qa_pair = {
                        'question': record['question'],
                        'answer': record['answer'],
                        'source': f"db_faq_{record['id']}",
                        'category': record.get('category'),
                        'keywords': [kw['keyword'] for kw in keywords],
                        'manual_keywords': [kw['keyword'] for kw in keywords if kw['type'] == 'manual'],
                        'auto_keywords': [kw['keyword'] for kw in keywords if kw['type'] == 'auto'],
                        'db_id': record['id'],
                        'view_count': record['view_count'],
                        'like_count': record['like_count'],
                        'is_manual': bool(record['is_manual'])
                    }

                    self.knowledge_base.append(qa_pair)

            conn.close()
            return len(faq_records)

        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½FAQå¤±è´¥: {str(e)}")
            return 0
    
    def _parse_knowledge_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        è§£æçŸ¥è¯†åº“æ–‡æ¡£ï¼Œæå–é—®ç­”å¯¹
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            List[Dict]: é—®ç­”å¯¹åˆ—è¡¨
        """
        qa_pairs = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é—®ç­”å¯¹ï¼ˆåŒ…å«å¯èƒ½çš„å…³é”®è¯ï¼‰
            # åŒ¹é…æ ¼å¼ï¼š### é—®ç­” N\n\n**é—®é¢˜ï¼š** ...\n\n**ç­”æ¡ˆï¼š** ...\n\n**å…³é”®è¯ï¼š** ...ï¼ˆå¯é€‰ï¼‰
            pattern = r'###\s*é—®ç­”\s*\d+\s*\n(.*?)(?=\n---|\n###|$)'
            
            matches = re.finditer(pattern, content, re.DOTALL | re.MULTILINE)
            
            for match in matches:
                qa_block = match.group(1)
                
                # æå–é—®é¢˜
                question_match = re.search(r'\*\*é—®é¢˜[ï¼š:]\*\*\s*(.*?)(?=\n\s*\*\*ç­”æ¡ˆ|\n\s*\*\*å…³é”®è¯|$)', qa_block, re.DOTALL)
                question = question_match.group(1).strip() if question_match else ""
                
                # æå–ç­”æ¡ˆ
                answer_match = re.search(r'\*\*ç­”æ¡ˆ[ï¼š:]\*\*\s*(.*?)(?=\n\s*\*\*å…³é”®è¯|\n\s*\*\*é—®é¢˜|$)', qa_block, re.DOTALL)
                answer = answer_match.group(1).strip() if answer_match else ""
                
                # æ¸…ç†æ ¼å¼
                question = re.sub(r'\*\*', '', question).strip()
                answer = re.sub(r'\*\*', '', answer).strip()
                
                if question and answer:
                    # å°è¯•ä»æ–‡æ¡£ä¸­æå–æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    manual_keywords = self._extract_manual_keywords(qa_block)
                    
                    # è‡ªåŠ¨æå–å…³é”®è¯
                    auto_keywords = self._extract_keywords(question + ' ' + answer)
                    
                    # åˆå¹¶å…³é”®è¯ï¼ˆæ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ä¼˜å…ˆçº§æ›´é«˜ï¼‰
                    all_keywords = list(set(manual_keywords + auto_keywords))
                    
                    qa_pairs.append({
                        'question': question,
                        'answer': answer,
                        'source': file_path.name,
                        'keywords': all_keywords[:15],  # æœ€å¤š15ä¸ªå…³é”®è¯
                        'manual_keywords': manual_keywords,  # æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯
                        'auto_keywords': auto_keywords  # è‡ªåŠ¨æå–çš„å…³é”®è¯
                    })
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æ¨¡å¼
            if not qa_pairs:
                # å°è¯•åŒ¹é…ï¼š**é—®é¢˜ï¼š** ... **ç­”æ¡ˆï¼š** ...
                pattern2 = r'(\*\*é—®é¢˜[ï¼š:]\*\*\s*.*?\s*\*\*ç­”æ¡ˆ[ï¼š:]\*\*\s*.*?)(?=\n---|\n###|$)'
                matches2 = re.finditer(pattern2, content, re.DOTALL | re.MULTILINE)
                
                for match in matches2:
                    qa_block = match.group(1)
                    
                    # æå–é—®é¢˜
                    question_match = re.search(r'\*\*é—®é¢˜[ï¼š:]\*\*\s*(.*?)(?=\n\s*\*\*ç­”æ¡ˆ|$)', qa_block, re.DOTALL)
                    question = question_match.group(1).strip() if question_match else ""
                    
                    # æå–ç­”æ¡ˆ
                    answer_match = re.search(r'\*\*ç­”æ¡ˆ[ï¼š:]\*\*\s*(.*?)(?=\n\s*\*\*å…³é”®è¯|$)', qa_block, re.DOTALL)
                    answer = answer_match.group(1).strip() if answer_match else ""
                    
                    # æ¸…ç†æ ¼å¼
                    question = re.sub(r'\*\*', '', question).strip()
                    answer = re.sub(r'\*\*', '', answer).strip()
                    
                    if question and answer:
                        # å°è¯•ä»æ–‡æ¡£ä¸­æå–æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        manual_keywords = self._extract_manual_keywords(qa_block)
                        
                        # è‡ªåŠ¨æå–å…³é”®è¯
                        auto_keywords = self._extract_keywords(question + ' ' + answer)
                        
                        # åˆå¹¶å…³é”®è¯ï¼ˆæ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ä¼˜å…ˆçº§æ›´é«˜ï¼‰
                        all_keywords = list(set(manual_keywords + auto_keywords))
                        
                        qa_pairs.append({
                            'question': question,
                            'answer': answer,
                            'source': file_path.name,
                            'keywords': all_keywords[:15],  # æœ€å¤š15ä¸ªå…³é”®è¯
                            'manual_keywords': manual_keywords,  # æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯
                            'auto_keywords': auto_keywords  # è‡ªåŠ¨æå–çš„å…³é”®è¯
                        })
            
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
        
        return qa_pairs
    
    def _extract_manual_keywords(self, text: str) -> List[str]:
        """
        ä»æ–‡æ¡£ä¸­æå–æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯
        
        æ”¯æŒæ ¼å¼ï¼š
        - **å…³é”®è¯ï¼š** èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        - **æ ‡ç­¾ï¼š** èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        - keywords: èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        
        Args:
            text: åŒ…å«é—®ç­”å¯¹çš„æ–‡æœ¬ç‰‡æ®µ
            
        Returns:
            List[str]: æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯åˆ—è¡¨
        """
        keywords = []
        
        # åŒ¹é…æ ¼å¼ï¼š**å…³é”®è¯ï¼š** èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        pattern1 = r'\*\*å…³é”®è¯[ï¼š:]\*\*\s*([^\n]+)'
        match1 = re.search(pattern1, text, re.IGNORECASE)
        if match1:
            keywords_str = match1.group(1).strip()
            keywords.extend([kw.strip() for kw in keywords_str.split(',') if kw.strip()])
        
        # åŒ¹é…æ ¼å¼ï¼š**æ ‡ç­¾ï¼š** èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        pattern2 = r'\*\*æ ‡ç­¾[ï¼š:]\*\*\s*([^\n]+)'
        match2 = re.search(pattern2, text, re.IGNORECASE)
        if match2:
            keywords_str = match2.group(1).strip()
            keywords.extend([kw.strip() for kw in keywords_str.split(',') if kw.strip()])
        
        # åŒ¹é…æ ¼å¼ï¼škeywords: èƒ°å²›ç´ ,å‰‚é‡,è®¡ç®—
        pattern3 = r'keywords[ï¼š:]\s*([^\n]+)'
        match3 = re.search(pattern3, text, re.IGNORECASE)
        if match3:
            keywords_str = match3.group(1).strip()
            keywords.extend([kw.strip() for kw in keywords_str.split(',') if kw.strip()])
        
        return list(set(keywords))  # å»é‡
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•çš„å…³é”®è¯æå–ï¼šå»é™¤åœç”¨è¯ï¼Œæå–é‡è¦è¯æ±‡
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                     'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                     'è‡ªå·±', 'è¿™', 'ä¸º', 'ä»€ä¹ˆ', 'èƒ½', 'å¯ä»¥', 'å¦‚ä½•', 'æ€ä¹ˆ', 'å¦‚æœ', 'éœ€è¦', 'åº”è¯¥'}
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ï¼‰
        words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
        
        # è¿‡æ»¤åœç”¨è¯å’Œé‡å¤è¯
        keywords = list(set([w for w in words if w not in stop_words and len(w) >= 2]))
        
        return keywords[:10]  # æœ€å¤šè¿”å›10ä¸ªå…³é”®è¯
    
    def _calculate_similarity(self, query: str, text: str, manual_keywords: List[str] = None) -> float:
        """
        è®¡ç®—æŸ¥è¯¢å’Œæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            text: ç›®æ ‡æ–‡æœ¬
            manual_keywords: æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = set(self._extract_keywords(query))
        
        # æå–æ–‡æœ¬å…³é”®è¯
        text_keywords = set(self._extract_keywords(text))
        
        # å¦‚æœæœ‰æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ï¼Œä¹ŸåŠ å…¥åŒ¹é…
        if manual_keywords:
            text_keywords.update(manual_keywords)
        
        if not query_keywords:
            return 0.0
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        intersection = query_keywords & text_keywords
        union = query_keywords | text_keywords
        
        if not union:
            return 0.0
        
        jaccard_score = len(intersection) / len(union)
        
        # è®¡ç®—å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å‡ºç°é¢‘ç‡
        query_text = query.lower()
        text_lower = text.lower()
        
        keyword_matches = sum(1 for kw in query_keywords if kw in text_lower)
        frequency_score = keyword_matches / len(query_keywords) if query_keywords else 0
        
        # æ‰‹åŠ¨å…³é”®è¯å‘½ä¸­åŠ åˆ†ï¼ˆå¦‚æœæŸ¥è¯¢å…³é”®è¯å‘½ä¸­æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ï¼Œç»™äºˆé¢å¤–åŠ åˆ†ï¼‰
        manual_hit_bonus = 0.0
        if manual_keywords:
            manual_keywords_set = set(manual_keywords)
            manual_hits = query_keywords & manual_keywords_set
            if manual_hits:
                # æ‰‹åŠ¨å…³é”®è¯å‘½ä¸­ç»™äºˆé¢å¤–0.2çš„åŠ åˆ†
                manual_hit_bonus = min(0.2, len(manual_hits) / len(query_keywords) * 0.3)
        
        # ç»¼åˆå¾—åˆ†ï¼ˆJaccard 50% + é¢‘ç‡ 30% + æ‰‹åŠ¨å…³é”®è¯å‘½ä¸­ 20%ï¼‰
        similarity = jaccard_score * 0.5 + frequency_score * 0.3 + manual_hit_bonus
        
        return min(1.0, similarity)  # ç¡®ä¿ä¸è¶…è¿‡1.0
    
    def search_knowledge(self, query: str, top_k: int = 3, min_similarity: float = 0.1) -> List[Dict[str, Any]]:
        """
        ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æœ€ç›¸å…³çš„top_kæ¡
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        if not self.is_loaded or not self.knowledge_base:
            logger.warning("âš ï¸ çŸ¥è¯†åº“æœªåŠ è½½æˆ–ä¸ºç©º")
            return []
        
        results = []
        
        # è®¡ç®—æ¯ä¸ªé—®ç­”å¯¹çš„ç›¸ä¼¼åº¦
        for qa in self.knowledge_base:
            # è·å–æ‰‹åŠ¨è®¾ç½®çš„å…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
            manual_keywords = qa.get('manual_keywords', [])
            
            # è®¡ç®—é—®é¢˜å’Œç­”æ¡ˆçš„ç›¸ä¼¼åº¦ï¼ˆä¼ å…¥æ‰‹åŠ¨å…³é”®è¯ï¼‰
            question_sim = self._calculate_similarity(query, qa['question'], manual_keywords)
            answer_sim = self._calculate_similarity(query, qa['answer'], manual_keywords)
            
            # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦ç›´æ¥å‘½ä¸­æ‰‹åŠ¨å…³é”®è¯ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            keyword_hit = False
            if manual_keywords:
                query_keywords = set(self._extract_keywords(query))
                manual_keywords_set = set(manual_keywords)
                if query_keywords & manual_keywords_set:
                    keyword_hit = True
                    # å¦‚æœå‘½ä¸­æ‰‹åŠ¨å…³é”®è¯ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
                    question_sim = min(1.0, question_sim + 0.15)
            
            # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆé—®é¢˜æƒé‡æ›´é«˜ï¼‰
            similarity = question_sim * 0.7 + answer_sim * 0.3
            
            if similarity >= min_similarity:
                results.append({
                    'question': qa['question'],
                    'answer': qa['answer'],
                    'similarity': similarity,
                    'source': qa['source'],
                    'keyword_hit': keyword_hit,  # æ˜¯å¦å‘½ä¸­æ‰‹åŠ¨å…³é”®è¯
                    'matched_keywords': list(set(self._extract_keywords(query)) & set(qa.get('keywords', []))) if keyword_hit else []  # åŒ¹é…åˆ°çš„å…³é”®è¯
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # è¿”å›top_kæ¡
        return results[:top_k]
    
    def answer_question(self, question: str, top_k: int = 3, use_ai: bool = False) -> Dict[str, Any]:
        """
        å›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢æœ€ç›¸å…³çš„top_kæ¡çŸ¥è¯†
            use_ai: æ˜¯å¦ä½¿ç”¨AIè¿›è¡Œç­”æ¡ˆç”Ÿæˆï¼ˆéœ€è¦é›†æˆDeepSeekï¼‰
            
        Returns:
            Dict: å›ç­”ç»“æœ
        """
        try:
            # ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³çŸ¥è¯†
            knowledge_results = self.search_knowledge(question, top_k=top_k)
            
            if not knowledge_results:
                return {
                    'success': False,
                    'answer': 'æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚',
                    'knowledge_used': [],
                    'confidence': 0.0
                }
            
            # å¦‚æœæ‰¾åˆ°ç›¸å…³çŸ¥è¯†
            best_match = knowledge_results[0]
            
            if use_ai:
                # ä½¿ç”¨AIç”Ÿæˆç­”æ¡ˆï¼ˆéœ€è¦é›†æˆDeepSeekæœåŠ¡ï¼‰
                # è¿™é‡Œå¯ä»¥è°ƒç”¨DeepSeek APIï¼Œå°†æ£€ç´¢åˆ°çš„çŸ¥è¯†ä½œä¸ºä¸Šä¸‹æ–‡
                answer = self._generate_ai_answer(question, knowledge_results)
            else:
                # ç›´æ¥è¿”å›æœ€ç›¸å…³çš„ç­”æ¡ˆ
                answer = best_match['answer']
            
            return {
                'success': True,
                'answer': answer,
                'knowledge_used': knowledge_results,
                'confidence': best_match['similarity'],
                'source': best_match['source']
            }
            
        except Exception as e:
            logger.error(f"âŒ å›ç­”é—®é¢˜å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'answer': f'å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}',
                'knowledge_used': [],
                'confidence': 0.0
            }
    
    def _generate_ai_answer(self, question: str, knowledge_results: List[Dict]) -> str:
        """
        ä½¿ç”¨AIç”Ÿæˆç­”æ¡ˆï¼ˆåŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            knowledge_results: æ£€ç´¢åˆ°çš„çŸ¥è¯†
            
        Returns:
            str: AIç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # TODO: é›†æˆDeepSeekæœåŠ¡ç”Ÿæˆç­”æ¡ˆ
        # è¿™é‡Œå¯ä»¥æ„å»ºpromptï¼Œå°†æ£€ç´¢åˆ°çš„çŸ¥è¯†ä½œä¸ºä¸Šä¸‹æ–‡
        # æš‚æ—¶è¿”å›æœ€ç›¸å…³çš„ç­”æ¡ˆ
        if knowledge_results:
            return knowledge_results[0]['answer']
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
    
    def get_knowledge_stats(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.is_loaded:
            return {
                'loaded': False,
                'total_qa': 0,
                'sources': []
            }
        
        # ç»Ÿè®¡æ¥æºæ–‡ä»¶
        sources = {}
        for qa in self.knowledge_base:
            source = qa.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1
        
        return {
            'loaded': True,
            'total_qa': len(self.knowledge_base),
            'sources': sources,
            'knowledge_base_path': str(self.knowledge_base_path)
        }


# å…¨å±€å•ä¾‹
_knowledge_qa_service_instance = None

def get_knowledge_qa_service(knowledge_base_path: Optional[str] = None) -> KnowledgeQAService:
    """è·å–çŸ¥è¯†é—®ç­”æœåŠ¡å•ä¾‹"""
    global _knowledge_qa_service_instance
    if _knowledge_qa_service_instance is None:
        _knowledge_qa_service_instance = KnowledgeQAService(knowledge_base_path)
    return _knowledge_qa_service_instance



